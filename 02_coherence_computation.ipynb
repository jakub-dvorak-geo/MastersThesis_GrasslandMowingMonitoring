{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac6a083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyroSAR\n",
    "from pyroSAR.snap.auxil import parse_recipe, parse_node\n",
    "from pyroSAR.snap.auxil import gpt, groupbyWorkers\n",
    "\n",
    "from os.path import join, exists, basename\n",
    "from os import makedirs\n",
    "from shutil import rmtree\n",
    "from glob import glob\n",
    "\n",
    "import warnings\n",
    "from time import perf_counter\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ab1a26-2165-4ca3-b3b2-ae223caff8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coherence_gpt(out_gpt, in_slc_one, in_slc_two, dem_path, out_filename, subswath, bursts, **kwargs):\n",
    "    \n",
    "    workflow = parse_recipe('blank')\n",
    "    \n",
    "    # 1st branch - preprocess first slc\n",
    "    read_one = parse_node('Read')\n",
    "    read_one.parameters['file'] = in_slc_one\n",
    "    workflow.insert_node(read_one)\n",
    "    \n",
    "    topsar_split_one = parse_node('TOPSAR-Split')\n",
    "    topsar_split_one.parameters['subswath'] = subswath\n",
    "    topsar_split_one.parameters['firstBurstIndex'] = bursts[0][0]\n",
    "    topsar_split_one.parameters['lastBurstIndex'] = bursts[0][1]\n",
    "    workflow.insert_node(topsar_split_one, before=read_one.id)\n",
    "    \n",
    "    apply_orbit_one = parse_node('Apply-Orbit-File')\n",
    "    apply_orbit_one.parameters['orbitType'] = 'Sentinel Precise (Auto Download)'\n",
    "    apply_orbit_one.parameters['polyDegree'] = 3\n",
    "    apply_orbit_one.parameters['continueOnFail'] = 'false'\n",
    "    workflow.insert_node(apply_orbit_one, before=topsar_split_one.id)\n",
    "    \n",
    "    # 2nd branch - preprocess second slc\n",
    "    read_two = parse_node('Read')\n",
    "    read_two.parameters['file'] = in_slc_two\n",
    "    workflow.insert_node(read_two)\n",
    "    \n",
    "    topsar_split_two = parse_node('TOPSAR-Split')\n",
    "    topsar_split_two.parameters['subswath'] = subswath\n",
    "    topsar_split_two.parameters['firstBurstIndex'] = bursts[1][0]\n",
    "    topsar_split_two.parameters['lastBurstIndex'] = bursts[1][1]\n",
    "    workflow.insert_node(topsar_split_two, before=read_two.id)\n",
    "    \n",
    "    apply_orbit_two = parse_node('Apply-Orbit-File')\n",
    "    apply_orbit_two.parameters['orbitType'] = 'Sentinel Precise (Auto Download)'\n",
    "    apply_orbit_two.parameters['polyDegree'] = 3\n",
    "    apply_orbit_two.parameters['continueOnFail'] = 'false'\n",
    "    workflow.insert_node(apply_orbit_two, before=topsar_split_two.id)\n",
    "    \n",
    "    # combined branch\n",
    "    # coregister preprocessed slcs from all three branches\n",
    "    back_geocoding = parse_node('Back-Geocoding')\n",
    "    back_geocoding.parameters['demName'] = 'External DEM'\n",
    "    back_geocoding.parameters['demResamplingMethod'] = 'BILINEAR_INTERPOLATION'\n",
    "    back_geocoding.parameters['externalDEMFile'] = dem_path\n",
    "    back_geocoding.parameters['externalDEMNoDataValue'] = 0.0\n",
    "    back_geocoding.parameters['resamplingType'] = 'BILINEAR_INTERPOLATION'\n",
    "    back_geocoding.parameters['maskOutAreaWithoutElevation'] = 'false'\n",
    "    back_geocoding.parameters['outputRangeAzimuthOffset'] = 'false'\n",
    "    back_geocoding.parameters['outputDerampDemodPhase'] = 'false'\n",
    "    back_geocoding.parameters['disableReramp'] = 'false'\n",
    "    workflow.insert_node(back_geocoding, before=[apply_orbit_one.id, apply_orbit_two.id])\n",
    "    \n",
    "    esd = parse_node('Enhanced-Spectral-Diversity')\n",
    "    workflow.insert_node(esd, before=back_geocoding.id)\n",
    "\n",
    "    # estimate coherence\n",
    "    coh = parse_node('Coherence')\n",
    "    #coh.parameters['squarePixel'] = 'true'\n",
    "    coh.parameters['cohWinAz'] = 5\n",
    "    coh.parameters['cohWinRg'] = 20\n",
    "    coh.parameters['subtractFlatEarthPhase'] = 'false'\n",
    "    coh.parameters['srpPolynomialDegree'] = 5\n",
    "    coh.parameters['srpNumberPoints'] = 501\n",
    "    coh.parameters['orbitDegree'] = 3\n",
    "    coh.parameters['subtractTopographicPhase'] = 'true'\n",
    "    coh.parameters['demName'] = 'External DEM'\n",
    "    #coherence.parameters['demResamplingMethod'] = 'BILINEAR_INTERPOLATION'\n",
    "    coh.parameters['externalDEMFile'] = dem_path\n",
    "    coh.parameters['externalDEMNoDataValue'] = 0.0\n",
    "    coh.parameters['externalDEMApplyEGM'] = 'true'\n",
    "    coh.parameters['tileExtensionPercent'] = 100\n",
    "    coh.parameters['singleMaster'] = 'false'\n",
    "    workflow.insert_node(coh, before=esd.id)\n",
    "\n",
    "    deburst = parse_node('TOPSAR-Deburst')\n",
    "    workflow.insert_node(deburst, before=coh.id)\n",
    "\n",
    "    tc = parse_node('Terrain-Correction')\n",
    "    tc.parameters['demName'] = 'External DEM'\n",
    "    tc.parameters['demResamplingMethod'] = 'BILINEAR_INTERPOLATION'\n",
    "    tc.parameters['externalDEMFile'] = dem_path\n",
    "    tc.parameters['externalDEMNoDataValue'] = 0.0\n",
    "    tc.parameters['externalDEMApplyEGM'] = 'true'\n",
    "\n",
    "    tc.parameters['imgResamplingMethod'] = 'BILINEAR_INTERPOLATION'\n",
    "    tc.parameters['pixelSpacingInMeter'] = 0.0\n",
    "    tc.parameters['pixelSpacingInDegree'] = 0.0\n",
    "    tc.parameters['mapProjection'] = '''GEOGCS[\"WGS 84\", DATUM[\"World Geodetic System 1984\",\n",
    "        SPHEROID[\"WGS 84\", 6378137.0, 298.257223563, AUTHORITY[\"EPSG\",\"7030\"]], \n",
    "        AUTHORITY[\"EPSG\",\"6326\"]], \n",
    "        PRIMEM[\"Greenwich\", 0.0, AUTHORITY[\"EPSG\",\"8901\"]], \n",
    "        UNIT[\"degree\", 0.017453292519943295], \n",
    "        AXIS[\"Geodetic longitude\", EAST], \n",
    "        AXIS[\"Geodetic latitude\", NORTH], \n",
    "        AUTHORITY[\"EPSG\",\"4326\"]]'''\n",
    "    # maybe? so everything is aligned\n",
    "    tc.parameters['alignToStandardGrid'] = 'true'\n",
    "    tc.parameters['standardGridOriginX'] = 0.0\n",
    "    tc.parameters['standardGridOriginY'] = 0.0\n",
    "    tc.parameters['nodataValueAtSea'] = 'false'\n",
    "    tc.parameters['saveDEM'] = 'false'\n",
    "    tc.parameters['saveLatLon'] = 'false'\n",
    "    tc.parameters['saveIncidenceAngleFromEllipsoid'] = 'false'\n",
    "    tc.parameters['saveLocalIncidenceAngle'] = 'false'\n",
    "    tc.parameters['saveProjectedLocalIncidenceAngle'] = 'false'\n",
    "    tc.parameters['saveSelectedSourceBand'] = 'true'\n",
    "    tc.parameters['saveLayoverShadowMask'] = 'false'\n",
    "    #tc.parameters['outputComplex'] = 'false'\n",
    "    tc.parameters['applyRadiometricNormalization'] = 'false'\n",
    "    tc.parameters['saveSigmaNought'] = 'false'\n",
    "    tc.parameters['saveGammaNought'] = 'false'\n",
    "    tc.parameters['saveBetaNought'] = 'false'\n",
    "    tc.parameters['incidenceAngleForSigma0'] = 'Use projected local incidence angle from DEM'\n",
    "    tc.parameters['incidenceAngleForGamma0'] = 'Use projected local incidence angle from DEM'\n",
    "    tc.parameters['auxFile'] = 'Latest Auxiliary File'\n",
    "    workflow.insert_node(tc, before=deburst.id)\n",
    "\n",
    "    write = parse_node('Write')\n",
    "    write.parameters['file'] = out_filename\n",
    "    write.parameters['formatName'] = 'GeoTIFF-BigTIFF'\n",
    "    workflow.insert_node(write, before=tc.id)\n",
    "\n",
    "    workflow.write(out_gpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f92e71-5231-4106-835d-799111d61307",
   "metadata": {},
   "source": [
    "### Run processing in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458527f4-bd9e-48c0-86cf-86691dbcfdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directiories used\n",
    "ROOT_DIR = '/media/sf_JD/DP'\n",
    "\n",
    "dtm_path = join(ROOT_DIR, 'DTM/DMR_4G_4326.tif')\n",
    "temp_dir = join(ROOT_DIR, 'temp')\n",
    "xmls_dir = join(temp_dir, 'snap_xmls')\n",
    "mst_dir  = join(temp_dir, 'masters')\n",
    "makedirs(xmls_dir) if not exists(xmls_dir) else print(f'{xmls_dir} already exists.')\n",
    "makedirs(mst_dir) if not exists(mst_dir) else print(f'{mst_dir} already exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e897ea79-a8a0-4814-9382-dfd2ec916f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "subswaths = {\n",
    "    22: 'IW2',\n",
    "    73: 'IW1',\n",
    "    95: 'IW1',\n",
    "    146:'IW3'\n",
    "}\n",
    "#### Suitable bursts for individual products\n",
    "bursts = {\n",
    "    2021: {\n",
    "        22: [\n",
    "            [6,7], [1,2], [6,7], [1,2], [6,7], [1,2], [6,7], [1,2], [1,2], [1,2],\n",
    "            [1,2], [1,2], [1,2], [1,2], [1,2], [1,2], [1,2], [1,2], [1,2], [1,2],\n",
    "            [1,2], [1,2], [1,2], [1,2], [1,2], [1,2], [1,2], [1,2], [1,2], [1,2]\n",
    "            ],\n",
    "        73: [[1,2]] * 30,\n",
    "        95: [\n",
    "            [1,2], [1,2], [5,6], [1,2], [5,6], [1,2], [5,6], [1,2], [1,2], [1,2],\n",
    "            [1,2], [1,2], [1,2], [1,2], [1,2], [1,2], [1,2], [1,2], [1,2], [1,2],\n",
    "            [1,2], [1,2], [1,2], [1,2], [1,2], [1,2], [1,2], [1,2], [1,2], [1,2]\n",
    "            ],\n",
    "        146: [[2,3]] * 30\n",
    "    },\n",
    "    2022: {\n",
    "        22: [[1,2]] * 15,\n",
    "        73: [[1,2]] * 15,\n",
    "        95: [[1,2]] * 13,\n",
    "        146: [[2,3]] * 15\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f5bd69-46f3-4d3f-ba1d-b3743fb0f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of data directories to process and corresponding list of parameters to the TOPSAR_split function\n",
    "years = (2022,)\n",
    "relative_orbs = (22, 73, 95, 146)\n",
    "\n",
    "# automate list creation based on years and ROs\n",
    "data_dir_list, out_dir_list, subswaths_list, bursts_list = [], [], [], []\n",
    "for year in years:\n",
    "    for relative_orb in relative_orbs:\n",
    "        data_dir_list.append(f'{ROOT_DIR}/s1_download/{year}/{relative_orb}')\n",
    "        out_dir_list.append(f'{ROOT_DIR}/s1_coherence/{year}/{relative_orb}')\n",
    "        subswaths_list.append(subswaths[relative_orb])\n",
    "        bursts_list.append(bursts[year][relative_orb])\n",
    "\n",
    "for out_dir in out_dir_list:\n",
    "    makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5d723d-95c7-4508-bae1-190054d3d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_a = perf_counter()\n",
    "\n",
    "for data_dir, out_dir, subswath, bursts in zip(data_dir_list, out_dir_list, subswaths_list, bursts_list):\n",
    "    \n",
    "    files_list = glob('S1*.zip', root_dir=data_dir)\n",
    "    files_list.sort(key=lambda x : x[17:25])\n",
    "    dates_list = (filename[17:25] for filename in files_list)\n",
    "    print(f'Working on datasets in folder {data_dir}')\n",
    "\n",
    "    \"\"\"\n",
    "    # preprocess master slc\n",
    "    master_in = join(data_dir, files_list[0])\n",
    "    master_gpt = join(xmls_dir, f'master_{basename(data_dir)}.xml')\n",
    "    master_preprocessed = join(mst_dir, f'master_{basename(data_dir)}')\n",
    "    create_master_gpt(master_gpt, master_in, master_preprocessed, subswath, bursts[0])\n",
    "    gpt(xmlfile=master_gpt, tmpdir=temp_dir)\n",
    "    print('Preprocessed common master image for coregistration.')\n",
    "    \"\"\"\n",
    "    \n",
    "    for idx in range(len(files_list)-1):\n",
    "        time_aa = perf_counter()\n",
    "        dates = (files_list[idx][17:25], files_list[idx+1][17:25])\n",
    "        print(f'Computing coherence between images from {dates[0]} and {dates[1]}')\n",
    "        \n",
    "        # check if the time differnece between computtaions is not too small\n",
    "        diff = int(str(datetime.strptime(dates[1], \"%Y%m%d\") - datetime.strptime(dates[0], \"%Y%m%d\")).split()[0])\n",
    "        if diff < 6:\n",
    "            warnings.warn(f'Computing coherence between observations only {diff} days apart. Is this correct?')\n",
    "    \n",
    "        graph_path = join(xmls_dir, f'coherence_{dates[0]}_{dates[1]}.xml')\n",
    "        in_filepath_one = join(data_dir, files_list[idx])\n",
    "        in_filepath_two = join(data_dir, files_list[idx+1])\n",
    "        out_path = join(out_dir, f'coh_{dates[0]}_{dates[1]}')\n",
    "    \n",
    "        create_coherence_gpt(graph_path, in_filepath_one, in_filepath_two, dtm_path, out_path, subswath, [bursts[idx], bursts[idx+1]], dates=dates)\n",
    "        gpt(xmlfile=graph_path, tmpdir=temp_dir, gpt_args=['-c', '20G'], groups=groupbyWorkers(graph_path, n=2))\n",
    "\n",
    "        print(f'Done in {(perf_counter() - time_aa)/60:.2f} minutes')\n",
    "rmtree(temp_dir)\n",
    "print(f'Finished computing all coherences in {(perf_counter()-time_a)/3600:.2f} hours.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1751c3-ce7b-47a4-95ee-9f11d9efb4b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
